{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d83bb-1765-4ce4-8e65-98148cf9673b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0, world 1): env://\n",
      "2024-04-03 00:37:37,126 [INFO] \n",
      "=====  Running Parameters    =====\n",
      "2024-04-03 00:37:37,126 [INFO] {\n",
      "    \"amp\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"dist_backend\": \"nccl\",\n",
      "    \"dist_url\": \"env://\",\n",
      "    \"distributed\": true,\n",
      "    \"evaluate\": false,\n",
      "    \"gpu\": 0,\n",
      "    \"init_lr\": 1e-05,\n",
      "    \"iters_per_epoch\": 4915,\n",
      "    \"job_name\": \"test\",\n",
      "    \"lr_sched\": \"linear_warmup_cosine_lr\",\n",
      "    \"max_epoch\": 50,\n",
      "    \"min_lr\": 1e-06,\n",
      "    \"num_workers\": 6,\n",
      "    \"output_dir\": \"outputs\",\n",
      "    \"rank\": 0,\n",
      "    \"resume_ckpt_path\": null,\n",
      "    \"seed\": 42,\n",
      "    \"task\": \"image_text_pretrain\",\n",
      "    \"train_splits\": [\n",
      "        \"train\"\n",
      "    ],\n",
      "    \"wandb_log\": true,\n",
      "    \"warmup_lr\": 1e-06,\n",
      "    \"warmup_steps\": 1000,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"world_size\": 1\n",
      "}\n",
      "2024-04-03 00:37:37,126 [INFO] \n",
      "======  Dataset Attributes  ======\n",
      "2024-04-03 00:37:37,126 [INFO] \n",
      "======== slake =======\n",
      "2024-04-03 00:37:37,127 [INFO] {\n",
      "    \"batch_size\": 4,\n",
      "    \"build_info\": {\n",
      "        \"ann_path\": \"/ibex/user/sabbam0a/MiniGPT-4/medvqa_dataset/Slake/Slake1.0/train_en.json\",\n",
      "        \"image_path\": \"/ibex/user/sabbam0a/MiniGPT-4/medvqa_dataset/Slake/Slake1.0/imgs\"\n",
      "    },\n",
      "    \"data_type\": \"images\",\n",
      "    \"sample_ratio\": 14,\n",
      "    \"text_processor\": {\n",
      "        \"train\": {\n",
      "            \"name\": \"blip_caption\"\n",
      "        }\n",
      "    },\n",
      "    \"vis_processor\": {\n",
      "        \"train\": {\n",
      "            \"image_size\": 224,\n",
      "            \"name\": \"blip2_image_train\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "2024-04-03 00:37:37,127 [INFO] \n",
      "======  Model Attributes  ======\n",
      "2024-04-03 00:37:37,127 [INFO] {\n",
      "    \"arch\": \"minigpt_v2\",\n",
      "    \"chat_template\": true,\n",
      "    \"ckpt\": \"\",\n",
      "    \"drop_path_rate\": 0,\n",
      "    \"end_sym\": \"</s>\",\n",
      "    \"freeze_vit\": true,\n",
      "    \"image_size\": 224,\n",
      "    \"llama_model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "    \"lora_alpha\": 16,\n",
      "    \"lora_r\": 64,\n",
      "    \"max_txt_len\": 1024,\n",
      "    \"model_type\": \"pretrain\",\n",
      "    \"prompt\": \"\",\n",
      "    \"use_grad_checkpoint\": true,\n",
      "    \"vit_precision\": \"fp16\"\n",
      "}\n",
      "2024-04-03 00:37:37,127 [INFO] Building datasets...\n",
      "2024-04-03 00:37:37,144 [INFO] Loading LLAMA\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.74it/s]\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n",
      "2024-04-03 00:37:41,968 [INFO] Loading LLAMA Done\n",
      "2024-04-03 00:37:41,968 [INFO] Loading VIT\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-04-03 00:37:42,971 [INFO] freeze vision encoder\n",
      "2024-04-03 00:37:42,971 [INFO] Loading VIT Done\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msabbagh\u001b[0m (\u001b[33mhaxpa\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/ibex/user/sabbam0a/grad/MiniGPT-4/wandb/run-20240403_003744-vr4qhh2j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT/runs/vr4qhh2j\u001b[0m\n",
      "2024-04-03 00:37:51,393 [INFO] Start training\n",
      "2024-04-03 00:37:52,981 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).\n",
      "2024-04-03 00:37:52,981 [INFO] Loaded 1103 records for train split from the dataset.\n",
      "batch sizes [[4]]\n",
      "module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "module.llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "module.llama_proj.weight\n",
      "module.llama_proj.bias\n",
      "2024-04-03 00:37:52,993 [INFO] number of trainable parameters: 49287168\n",
      "2024-04-03 00:37:52,994 [INFO] Start training epoch 0, 4915 iters per inner epoch.\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Train: data epoch: [0]  [   0/4915]  eta: 3:46:57  lr: 0.000001  loss: 8.7019  time: 2.7706  data: 0.0000  max mem: 41000\n",
      "Train: data epoch: [0]  [  50/4915]  eta: 0:16:29  lr: 0.000001  loss: 7.0824  time: 0.1438  data: 0.0000  max mem: 42290\n",
      "Train: data epoch: [0]  [ 100/4915]  eta: 0:13:52  lr: 0.000002  loss: 5.0050  time: 0.1418  data: 0.0000  max mem: 42290\n",
      "Train: data epoch: [0]  [ 150/4915]  eta: 0:12:50  lr: 0.000002  loss: 3.9357  time: 0.1403  data: 0.0000  max mem: 42290\n",
      "Train: data epoch: [0]  [ 200/4915]  eta: 0:12:18  lr: 0.000003  loss: 2.5734  time: 0.1436  data: 0.0000  max mem: 42290\n",
      "Train: data epoch: [0]  [ 250/4915]  eta: 0:11:56  lr: 0.000003  loss: 1.4008  time: 0.1387  data: 0.0000  max mem: 42290\n",
      "Train: data epoch: [0]  [ 300/4915]  eta: 0:12:12  lr: 0.000004  loss: 1.1803  time: 0.1401  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 350/4915]  eta: 0:11:51  lr: 0.000004  loss: 2.4774  time: 0.1402  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 400/4915]  eta: 0:11:35  lr: 0.000005  loss: 2.3185  time: 0.1410  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 450/4915]  eta: 0:11:21  lr: 0.000005  loss: 0.3452  time: 0.1384  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 500/4915]  eta: 0:11:07  lr: 0.000005  loss: 2.3611  time: 0.1398  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 550/4915]  eta: 0:11:14  lr: 0.000006  loss: 2.3433  time: 0.2511  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 600/4915]  eta: 0:11:01  lr: 0.000006  loss: 0.6521  time: 0.1395  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 650/4915]  eta: 0:10:49  lr: 0.000007  loss: 1.9755  time: 0.1396  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 700/4915]  eta: 0:10:38  lr: 0.000007  loss: 1.2349  time: 0.1396  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 750/4915]  eta: 0:10:27  lr: 0.000008  loss: 1.0447  time: 0.1423  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 800/4915]  eta: 0:10:17  lr: 0.000008  loss: 0.3751  time: 0.1413  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 850/4915]  eta: 0:10:18  lr: 0.000009  loss: 0.2959  time: 0.1404  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 900/4915]  eta: 0:10:08  lr: 0.000009  loss: 0.3926  time: 0.1406  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [ 950/4915]  eta: 0:09:58  lr: 0.000010  loss: 0.5894  time: 0.1414  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1000/4915]  eta: 0:09:49  lr: 0.000010  loss: 0.4950  time: 0.1461  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1050/4915]  eta: 0:09:39  lr: 0.000010  loss: 0.5527  time: 0.1385  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1100/4915]  eta: 0:09:38  lr: 0.000010  loss: 0.4981  time: 0.2502  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1150/4915]  eta: 0:09:28  lr: 0.000010  loss: 0.9272  time: 0.1415  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1200/4915]  eta: 0:09:19  lr: 0.000010  loss: 0.2408  time: 0.1403  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1250/4915]  eta: 0:09:10  lr: 0.000010  loss: 0.2559  time: 0.1406  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1300/4915]  eta: 0:09:01  lr: 0.000010  loss: 0.3347  time: 0.1406  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1350/4915]  eta: 0:08:52  lr: 0.000010  loss: 0.1827  time: 0.1395  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1400/4915]  eta: 0:08:49  lr: 0.000010  loss: 0.1605  time: 0.1365  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1450/4915]  eta: 0:08:40  lr: 0.000010  loss: 0.4054  time: 0.1371  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1500/4915]  eta: 0:08:31  lr: 0.000010  loss: 0.5123  time: 0.1394  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1550/4915]  eta: 0:08:23  lr: 0.000010  loss: 0.3402  time: 0.1384  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1600/4915]  eta: 0:08:14  lr: 0.000010  loss: 1.1170  time: 0.1381  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1650/4915]  eta: 0:08:10  lr: 0.000010  loss: 0.3711  time: 0.2530  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1700/4915]  eta: 0:08:02  lr: 0.000010  loss: 0.1447  time: 0.1430  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1750/4915]  eta: 0:07:54  lr: 0.000010  loss: 0.1949  time: 0.1440  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1800/4915]  eta: 0:07:46  lr: 0.000010  loss: 0.5096  time: 0.1395  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1850/4915]  eta: 0:07:38  lr: 0.000010  loss: 0.2124  time: 0.1418  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1900/4915]  eta: 0:07:30  lr: 0.000010  loss: 0.6077  time: 0.1424  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [1950/4915]  eta: 0:07:25  lr: 0.000010  loss: 0.2754  time: 0.1393  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2000/4915]  eta: 0:07:17  lr: 0.000010  loss: 0.9777  time: 0.1433  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2050/4915]  eta: 0:07:09  lr: 0.000010  loss: 0.0199  time: 0.1415  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2100/4915]  eta: 0:07:01  lr: 0.000010  loss: 0.0928  time: 0.1398  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2150/4915]  eta: 0:06:52  lr: 0.000010  loss: 0.0167  time: 0.1402  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2200/4915]  eta: 0:06:47  lr: 0.000010  loss: 0.1085  time: 0.2505  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2250/4915]  eta: 0:06:39  lr: 0.000010  loss: 0.1836  time: 0.1410  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2300/4915]  eta: 0:06:31  lr: 0.000010  loss: 0.0445  time: 0.1423  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2350/4915]  eta: 0:06:23  lr: 0.000010  loss: 0.0751  time: 0.1408  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2400/4915]  eta: 0:06:15  lr: 0.000010  loss: 0.7180  time: 0.1415  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2450/4915]  eta: 0:06:07  lr: 0.000010  loss: 0.1719  time: 0.1397  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2500/4915]  eta: 0:06:01  lr: 0.000010  loss: 0.0808  time: 0.1420  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2550/4915]  eta: 0:05:54  lr: 0.000010  loss: 0.5068  time: 0.1394  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2600/4915]  eta: 0:05:46  lr: 0.000010  loss: 0.2874  time: 0.1413  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2650/4915]  eta: 0:05:38  lr: 0.000010  loss: 0.0280  time: 0.1386  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2700/4915]  eta: 0:05:30  lr: 0.000010  loss: 0.1213  time: 0.1432  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2750/4915]  eta: 0:05:24  lr: 0.000010  loss: 1.4206  time: 0.2523  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2800/4915]  eta: 0:05:16  lr: 0.000010  loss: 0.2244  time: 0.1420  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2850/4915]  eta: 0:05:08  lr: 0.000010  loss: 0.0716  time: 0.1406  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2900/4915]  eta: 0:05:00  lr: 0.000010  loss: 0.2921  time: 0.1417  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [2950/4915]  eta: 0:04:53  lr: 0.000010  loss: 0.1971  time: 0.1434  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [3000/4915]  eta: 0:04:45  lr: 0.000010  loss: 0.1206  time: 0.1452  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [3050/4915]  eta: 0:04:39  lr: 0.000010  loss: 0.0488  time: 0.1380  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [3100/4915]  eta: 0:04:31  lr: 0.000010  loss: 0.0303  time: 0.1424  data: 0.0000  max mem: 42888\n",
      "Train: data epoch: [0]  [3150/4915]  eta: 0:04:23  lr: 0.000010  loss: 0.1810  time: 0.1411  data: 0.0000  max mem: 42888\n"
     ]
    }
   ],
   "source": [
    "!torchrun train.py --cfg-path train_configs/minigptv2_finetune.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "574cec22-252b-45d2-8303-08e85122b2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n",
      "Position interpolate from 16x16 to 32x32\n",
      "Load Minigpt-4-LLM Checkpoint: minigpt4/outputs/D_slake_A_minigptv2/checkpoint_0.pth\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://81c7555ab01af10840.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://81c7555ab01af10840.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!python /ibex/user/sabbam0a/grad/MiniGPT-4/demo_v2.py --cfg-path /ibex/user/sabbam0a/grad/MiniGPT-4/eval_configs/minigptv2_eval.yaml --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56aa259-4c25-4b45-8c87-899c3ac5ea77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
